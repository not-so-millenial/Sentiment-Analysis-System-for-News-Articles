{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed296721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies (if running for the first time)\n",
    "!pip install requests html5lib beautifulsoup4 pandas numpy nltk seaborn matplotlib transformers gensim pyLDAvis spacy\n",
    "!python -m nltk.downloader punkt stopwords averaged_perceptron_tagger wordnet\n",
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83c2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "import pyLDAvis.gensim_models\n",
    "import pyLDAvis\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd4bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_news_links(query):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    url = f'https://www.google.com/search?q={query}&tbm=nws'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [a['href'][7:].split('&')[0] for a in soup.find_all('a', href=True) if 'url?q=' in a['href']]\n",
    "    return links\n",
    "\n",
    "query = \"modi\"\n",
    "links = get_news_links(query)\n",
    "df_links = pd.DataFrame(links, columns=[\"Links\"])\n",
    "df_links.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b0fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        return ' '.join([para.get_text() for para in paragraphs])\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "texts = [fetch_article_text(url) for url in links[:5]]  # Use top 5 links for speed\n",
    "df = pd.DataFrame({'url': links[:5], 'text': texts})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8445f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "df['tokens'] = df['text'].apply(preprocess)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "df['entities'] = df['text'].apply(extract_entities)\n",
    "df['pos_tags'] = df['tokens'].apply(nltk.pos_tag)\n",
    "df[['entities', 'pos_tags']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63be4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def get_sentiment(text):\n",
    "    try:\n",
    "        result = sentiment_pipeline(text[:512])[0]\n",
    "        return result['label'], result['score']\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "df[['sentiment', 'score']] = df['text'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "df[['text', 'sentiment', 'score']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e9493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['tokens']]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=10)\n",
    "\n",
    "topics = lda_model.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "vis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef78178",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.countplot(data=df, x='sentiment')\n",
    "plt.title(\"Sentiment Distribution of Articles\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
